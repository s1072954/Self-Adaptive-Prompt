{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 你的LLM api 有溫度的用來生prompt才不會一直重複 沒溫度的用來評估prompt才不會不公平\n",
    "from api.Mistral2_temperature_05_API import get_temperature_mistral\n",
    "from api.Mistral2 import get_mistral\n",
    "\n",
    "# 你的訓練資料集 我使用100筆 所以分數是0~100\n",
    "training_data=[]\n",
    "\n",
    "# 評估prompt的分數\n",
    "def get_score(new_prompt,training_data):\n",
    "   \"\"\"\n",
    "   將你練蠱時用來測試新prompt的data在這跑，之後回傳他的分數\n",
    "   \"\"\"\n",
    "   score=get_mistral()\n",
    "   return score\n",
    "\n",
    "# 練蠱終止條件(我是設超過baseline做100題後的分數:60)\n",
    "stop_score=60\n",
    "\n",
    "# 起始prompt與分數(自己給幾個簡單prompt拿去get_score跑)\n",
    "prompt_list=[\n",
    "  {'prompt': \"Let's think step by step.\", \n",
    "   'score': 48},\n",
    "  {'prompt': '', \n",
    "   'score': 46},\n",
    "  {'prompt': ' \"Identify the specific information that responds to the question in the article.\"',\n",
    "   'score': 50}]\n",
    "\n",
    "# 你要給LLM看幾個example\n",
    "example_num=5\n",
    "\n",
    "# 開練\n",
    "while sorted_prompt_list[0]['score']<stop_score:\n",
    "  # 單純用來終止迴圈 你也可以interrupt\n",
    "  if os.path.exists(os.path.join(os.getcwd(),'stop_true.txt')):\n",
    "      break\n",
    "  \n",
    "  # 排序你的prompt 拿比較高分的給LLM當example\n",
    "  sorted_prompt_list = sorted(prompt_list, key=lambda x: x['score'], reverse=True)\n",
    "  # 整理example格式\n",
    "  example=\"\"\n",
    "  for p in sorted_prompt_list[:example_num]:\n",
    "      example+=f\"\"\"[Old prompt]:\"{p['prompt']}\"\n",
    "  [Scores]:{p['score']}\n",
    "\n",
    "  \"\"\"\n",
    "  # 整理輸入(可以print出來檢查)\n",
    "  input_to_temperature_llm=f\"\"\"You are an expert at crafting prompts.\n",
    "  Based on the example task given below for an LLM, fill in the most suitable prompt in the place marked [new_prompt].\n",
    "  The following describes the task you will undertake:\n",
    "\n",
    "  \"\n",
    "  Article excerpt:\n",
    "  [article_chunk]\n",
    "\n",
    "  The above is the article excerpt related to my question.\n",
    "  Below is the question I want to ask.\n",
    "  Please select the text content that can answer this question.\n",
    "  [new_prompt]\n",
    "\n",
    "  Question:\n",
    "  [input_question]\n",
    "  \"\n",
    "\n",
    "  Here are some example prompts and their scores, ranging from 0 to 100, with higher scores indicating better performance.\n",
    "  Please help me think of a unique new_prompt where higher scores are better.\n",
    "\n",
    "  {example}\n",
    "\n",
    "  ### You only need to return the new_prompt ###\n",
    "  DON'T return the [Scores] or explanation.\n",
    "  Your new_prompt:__\"\"\"\n",
    "  new_prompt=get_temperature_mistral(input_to_temperature_llm)\n",
    "  \n",
    "  score=get_score(new_prompt,training_data)\n",
    "  print(\"*\"*50)\n",
    "  print(f\"{new_prompt}\\n{score}\")\n",
    "  prompt_list.append({\"prompt\":new_prompt,\n",
    "      \"score\":score})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
